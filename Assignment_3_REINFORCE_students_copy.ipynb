{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXV_DmegYlC5"
      },
      "source": [
        "# Introduction\n",
        "For this part of the assignment, we are going to use the corridor environment, which has been defined in this notebook. The corridor is shown in the graph below. The reward is -1 per step as usual. The grey block is the terminal state. In each of the three nonterminal states there are only two actions, ***right*** and ***left***. These actions have their usual consequences in the first and third states (left causes no movement in the first state), but in the second state they are reversed, so that right moves to the left and left moves to the right.\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "We are going to implment the REINFORCE algorithm and evaluate it on this environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HcaweCH7vlwi"
      },
      "outputs": [],
      "source": [
        "#######################################################################\n",
        "# Copyright (C)                                                       #\n",
        "# 2018 Sergii Bondariev (sergeybondarev@gmail.com)                    #\n",
        "# 2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
        "# Permission given to modify the code as long as you keep this        #\n",
        "# declaration at the top                                              #\n",
        "#######################################################################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "FRESH_TIME = 0.3    # fresh time for one move\n",
        "\n",
        "def true_value(p):\n",
        "    \"\"\" True value of the first state\n",
        "    Args:\n",
        "        p (float): probability of the action 'right'.\n",
        "    Returns:\n",
        "        True value of the first state.\n",
        "        The expression is obtained by manually solving the easy linear system\n",
        "        of Bellman equations using known dynamics.\n",
        "    \"\"\"\n",
        "    return (2 * p - 4) / (p * (1 - p)) #v(s)\n",
        "\n",
        "N_STATES = 4\n",
        "Render = False\n",
        "\n",
        "class ShortCorridor:\n",
        "    \"\"\"\n",
        "    Short corridor environment\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "\n",
        "    def step(self, go_right):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            go_right (bool): chosen action\n",
        "        Returns:\n",
        "            tuple of (reward, episode terminated?)\n",
        "        \"\"\"\n",
        "        if self.state == 0 or self.state == 2:\n",
        "            if go_right:\n",
        "                self.state += 1\n",
        "            else:\n",
        "                self.state = max(0, self.state - 1)\n",
        "        else:\n",
        "            if go_right:\n",
        "                self.state -= 1\n",
        "            else:\n",
        "                self.state += 1\n",
        "\n",
        "        if self.state == 3:\n",
        "            # terminal state\n",
        "            return 0, True  # reward\n",
        "        else:\n",
        "            return -1, False\n",
        "        \n",
        "    def render(self,episode,step_counter):\n",
        "        env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
        "        if self.state == 3:\n",
        "            interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
        "            print('\\r{}'.format(interaction), end='')\n",
        "            time.sleep(2)\n",
        "            print('\\r                                ', end='')\n",
        "        else:\n",
        "            env_list[self.state] = 'o'\n",
        "            interaction = ''.join(env_list)\n",
        "            print('\\r{}'.format(interaction), end='')\n",
        "            time.sleep(FRESH_TIME)\n",
        "    \n",
        "\n",
        "def softmax(x):\n",
        "    t = np.exp(x - np.max(x))\n",
        "    return t / np.sum(t)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FavfDYrRwxK0"
      },
      "source": [
        "**Question 1: REINFORCE algorithm**\n",
        "\n",
        "In this part, you need to implement the REINFORCE algorithm. You will need to do the following:        \n",
        "(1) Initialize a differentiable policy parameterized by $\\theta$.                 \n",
        "(2) Generate an episode with policy. Keep rolling out to get new state s1, reward r and done d from environment until the terminate state.\n",
        "\n",
        "(3) Calculate returns $G$ for each step of the episode $t = 0, 1, \\ldots, T-1$.                         \n",
        "(4) Update policy.                                                          \n",
        "\n",
        "Enter your code in the block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "hXcmsdkKvvhg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:  100 rewards:  -9.86\n",
            "Episode:  200 rewards:  -8.02\n",
            "Episode:  300 rewards:  -8.39\n",
            "Episode:  400 rewards:  -7.62\n",
            "Episode:  500 rewards:  -7.28\n",
            "Episode:  600 rewards:  -6.63\n",
            "Episode:  700 rewards:  -5.79\n",
            "Episode:  800 rewards:  -5.65\n",
            "Episode:  900 rewards:  -5.91\n",
            "Episode:  1000 rewards:  -5.55\n",
            "Episode:  1100 rewards:  -4.65\n",
            "Episode:  1200 rewards:  -5.29\n",
            "Episode:  1300 rewards:  -5.35\n",
            "Episode:  1400 rewards:  -5.04\n",
            "Episode:  1500 rewards:  -4.72\n",
            "Episode:  1600 rewards:  -4.96\n",
            "Episode:  1700 rewards:  -5.21\n",
            "Episode:  1800 rewards:  -6.16\n",
            "Episode:  1900 rewards:  -5.94\n",
            "Episode:  2000 rewards:  -5.68\n",
            "Episode:  2100 rewards:  -5.65\n",
            "Episode:  2200 rewards:  -6.27\n",
            "Episode:  2300 rewards:  -6.42\n",
            "Episode:  2400 rewards:  -5.86\n",
            "Episode:  2500 rewards:  -5.68\n",
            "Episode:  2600 rewards:  -6.57\n",
            "Episode:  2700 rewards:  -6.16\n",
            "Episode:  2800 rewards:  -5.55\n",
            "Episode:  2900 rewards:  -6.23\n",
            "Episode:  3000 rewards:  -7.43\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class PolicyNet(torch.nn.Module):\n",
        "    def __init__(self,obs_dim,act_dim,hidden_dim):\n",
        "        super(PolicyNet,self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(obs_dim,hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim,act_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "        y = F.relu(self.fc1(x))\n",
        "        return self.fc2(y)\n",
        "\n",
        "class ReinforceAgent:\n",
        "    \"\"\"\n",
        "    ReinforceAgent that follows algorithm\n",
        "    'REINFORNCE Monte-Carlo Policy-Gradient Control (episodic)'\n",
        "    \"\"\"\n",
        "\n",
        "        ##############################################################################\n",
        "        #                              ENTER YOUR CODE                               #\n",
        "        ##############################################################################\n",
        "\n",
        "    def __init__(self):\n",
        "        # 环境的状态和动作维度\n",
        "        self.obs_dim = 1\n",
        "        self.act_dim = 2\n",
        "        self.hidden_dim = 16\n",
        "        self.gamma = 0.8\n",
        "        self.device =torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
        "    \"cpu\")\n",
        "        # 存放每个episode的s,a,r\n",
        "        self.ep_obs,self.ep_act,self.ep_r = [],[],[]\n",
        "        # 初始化神经网络\n",
        "        self.actor = PolicyNet(obs_dim = self.obs_dim,act_dim = self.act_dim, hidden_dim = self.hidden_dim).to(self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
        "        # self.time_step = 0\n",
        "        \n",
        "    \n",
        "    def choose_action(self,obs):\n",
        "        obs = torch.tensor([obs],dtype=torch.float).to(self.device)   # 转换为torch的格式\n",
        "        prob = F.softmax(self.actor(obs),dim=0)\n",
        "        action = np.random.choice(range(self.act_dim),p=prob.detach().cpu().numpy())   # 根据softmax输出的概率来选择动作\n",
        "        return action\n",
        "    \n",
        "    # 存储一个episode的状态、动作和回报的函数\n",
        "    def store_transition(self,obs,act,r):\n",
        "        self.ep_obs.append(obs)\n",
        "        self.ep_act.append(act)\n",
        "        self.ep_r.append(r)\n",
        "\n",
        "    # 更新策略网络的函数\n",
        "    def update(self):\n",
        "        # self.time_step += 1  # 记录走过的step\n",
        "        # 记录Gt的值\n",
        "        discounted_ep_rs = np.zeros_like(self.ep_r,dtype = float)\n",
        "        running_add = 0\n",
        "        # 计算未来总收益\n",
        "        for t in reversed(range(0,len(self.ep_r))):  # 反向计算\n",
        "            running_add = running_add * self.gamma + self.ep_r[t]\n",
        "            discounted_ep_rs[t] = running_add\n",
        "\n",
        "        discounted_ep_rs -= np.mean(discounted_ep_rs)  # 减均值\n",
        "        discounted_ep_rs /= np.std(discounted_ep_rs)  # 除以标准差\n",
        "        discounted_ep_rs = torch.FloatTensor(discounted_ep_rs).to(self.device)\n",
        "\n",
        "        # 输出网络计算出的每个动作的概率值\n",
        "        act_logit = self.actor(torch.FloatTensor(self.ep_obs).reshape(-1,1).to(self.device))\n",
        "        \n",
        "        # 进行交叉熵的运算\n",
        "        neg_log_prob = F.cross_entropy(input = act_logit,target=torch.LongTensor(self.ep_act).to(self.device),reduction = 'none')\n",
        "        # neg_log_prob = torch.sum(-F.log_softmax(act_logit) * torch.nn.functional.one_hot(torch.LongTensor(self.ep_act).to(self.device), self.act_dim), dim=1)\n",
        "        # 计算loss\n",
        "        loss = torch.mean(neg_log_prob * discounted_ep_rs)\n",
        "\n",
        "        # 反向传播优化网络\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # 每次学习后清空s,r,a的数组\n",
        "        self.ep_r,self.ep_act,self.ep_obs = [],[],[]\n",
        "\n",
        "\n",
        "EPISODE = 3000\n",
        "RENDER = False\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = ShortCorridor()\n",
        "    # 初始化ReinforceAgent类\n",
        "    agent = ReinforceAgent()\n",
        "    # 进行训练\n",
        "    reward_total =[]\n",
        "    for episode in range(EPISODE):\n",
        "            env.reset()\n",
        "            obs = env.state\n",
        "            \n",
        "            rewards = 0\n",
        "            step_count = 0\n",
        "            while True:\n",
        "                # 与环境的交互\n",
        "                step_count += 1\n",
        "                action = agent.choose_action(obs)\n",
        "                reward,done = env.step(action)\n",
        "                next_obs = env.state\n",
        "                # 存储一个episode中每个step的s,a,r\n",
        "                agent.store_transition(obs,action,reward)\n",
        "                # 进入下一个状态\n",
        "                obs = next_obs\n",
        "                rewards += reward\n",
        "                # 每个episode结束再进行训练(MC)\n",
        "                if RENDER and (episode+1) % 100 == 0 and episode !=0:\n",
        "                    env.render(episode,step_count)\n",
        "                if done:\n",
        "                    agent.update()\n",
        "                    break     \n",
        "            reward_total.append(rewards)\n",
        "            # 每100个episode进行测试\n",
        "            if (episode+1) % 100 == 0 and episode !=0:\n",
        "                avg_reward = np.mean(reward_total[-100:])\n",
        "                print('Episode: ',episode+1,'rewards: ',avg_reward)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dds8UjsxbZZ"
      },
      "source": [
        "**Question 2: REINFORCE with baseline (episodic)**\n",
        "\n",
        "In this section, you need to implement another version of REINFORCE with baseline. Addition to the implementation in vanilla REINFORCE, you need to add a differentiable state-value function w, which is initialized as self.w = 0. You need to use this w as the baseline to reduce the variance of REINFORCE by replacing $G$ with advantage $\\delta = G - w$. Then you can update w and $\\theta$ as\n",
        "\n",
        "$\\begin{aligned}\n",
        "& \\mathbf{w} \\leftarrow \\mathbf{w}+\\alpha^{\\mathbf{w}} \\delta \\nabla \\hat{v}\\left(S_t, \\mathbf{w}\\right) \\\\\n",
        "& \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}+\\alpha^{\\boldsymbol{\\theta}} \\gamma^t \\delta \\nabla \\ln \\pi\\left(A_t \\mid S_t, \\boldsymbol{\\theta}\\right)\n",
        "\\end{aligned}$.\n",
        "\n",
        "For more details about the algorithm, refer to the lecture notes LN17 or Chapter 13 \"REINFORCE with Baseline (episodic)\" in the book \"Reinforcement learning: an introduction\" by Richard Sutton.\n",
        "\n",
        "Enter your code in the block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "YlfbRDsAxbk1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:  100 rewards:  -13.78\n",
            "Episode:  200 rewards:  -13.51\n",
            "Episode:  300 rewards:  -12.87\n",
            "Episode:  400 rewards:  -13.33\n",
            "Episode:  500 rewards:  -10.64\n",
            "Episode:  600 rewards:  -9.11\n",
            "Episode:  700 rewards:  -8.78\n",
            "Episode:  800 rewards:  -8.38\n",
            "Episode:  900 rewards:  -9.2\n",
            "Episode:  1000 rewards:  -7.73\n",
            "Episode:  1100 rewards:  -6.87\n",
            "Episode:  1200 rewards:  -8.08\n",
            "Episode:  1300 rewards:  -6.95\n",
            "Episode:  1400 rewards:  -6.44\n",
            "Episode:  1500 rewards:  -6.19\n",
            "Episode:  1600 rewards:  -5.85\n",
            "Episode:  1700 rewards:  -5.64\n",
            "Episode:  1800 rewards:  -4.08\n",
            "Episode:  1900 rewards:  -4.23\n",
            "Episode:  2000 rewards:  -4.0\n",
            "Episode:  2100 rewards:  -4.32\n",
            "Episode:  2200 rewards:  -4.53\n",
            "Episode:  2300 rewards:  -3.75\n",
            "Episode:  2400 rewards:  -3.67\n",
            "Episode:  2500 rewards:  -3.44\n",
            "Episode:  2600 rewards:  -3.78\n",
            "Episode:  2700 rewards:  -3.56\n",
            "Episode:  2800 rewards:  -3.39\n",
            "Episode:  2900 rewards:  -3.25\n",
            "Episode:  3000 rewards:  -3.12\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class PolicyNet(torch.nn.Module):\n",
        "    def __init__(self,obs_dim,act_dim,hidden_dim):\n",
        "        super(PolicyNet,self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(obs_dim,hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim,act_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "        y = F.relu(self.fc1(x))\n",
        "        return self.fc2(y)\n",
        "\n",
        "class ValueNet(torch.nn.Module):\n",
        "    def __init__(self,obs_dim,hidden_dim):\n",
        "        super(ValueNet,self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(obs_dim,hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        y = F.relu(self.fc1(x))\n",
        "        return self.fc2(y)\n",
        "\n",
        "\n",
        "class ReinforceBaselineAgent():\n",
        "        ##############################################################################\n",
        "        #                             ENTER YOUR CODE                               #\n",
        "        ##############################################################################\n",
        "    def __init__(self):\n",
        "        # 环境的状态和动作维度\n",
        "        self.obs_dim = 1\n",
        "        self.act_dim = 2\n",
        "        self.hidden_dim = 16\n",
        "        self.gamma = 0.9\n",
        "        self.device =torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
        "    \"cpu\")\n",
        "        # 存放每个episode的s,a,r\n",
        "        self.ep_obs,self.ep_act,self.ep_r = [],[],[]\n",
        "        # 初始化神经网络\n",
        "        self.actor = PolicyNet(obs_dim = self.obs_dim,act_dim = self.act_dim, hidden_dim = self.hidden_dim).to(self.device)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
        "        self.value = ValueNet(obs_dim = self.obs_dim,hidden_dim = self.hidden_dim).to(self.device)\n",
        "        self.value_optimizer = torch.optim.Adam(self.value.parameters(), lr=1e-3)\n",
        "\n",
        "        # self.time_step = 0\n",
        "        \n",
        "    \n",
        "    def choose_action(self,obs):\n",
        "        obs = torch.tensor([obs],dtype=torch.float).to(self.device)   # 转换为torch的格式\n",
        "        prob = F.softmax(self.actor(obs),dim=0)\n",
        "        action = np.random.choice(range(self.act_dim),p=prob.detach().cpu().numpy())   # 根据softmax输出的概率来选择动作\n",
        "        return action\n",
        "    \n",
        "    # 存储一个episode的状态、动作和回报的函数\n",
        "    def store_transition(self,obs,act,r):\n",
        "        self.ep_obs.append(obs)\n",
        "        self.ep_act.append(act)\n",
        "        self.ep_r.append(r)\n",
        "\n",
        "    # 更新策略网络的函数\n",
        "    def update(self):\n",
        "        # self.time_step += 1  # 记录走过的step\n",
        "        # 记录Gt的值\n",
        "        obs = torch.FloatTensor(self.ep_obs).reshape(-1,1).to(self.device)\n",
        "        discounted_ep_rs = np.zeros_like(self.ep_r,dtype = float)\n",
        "        running_add = 0\n",
        "        # 计算未来总收益\n",
        "        for t in reversed(range(0,len(self.ep_r))):  # 反向计算\n",
        "            running_add = running_add * self.gamma + self.ep_r[t]\n",
        "            discounted_ep_rs[t] = running_add\n",
        "        v = self.value(obs)\n",
        "        # v = (v-v.mean())/(v.std()+1e-5)\n",
        "        discounted_ep_rs = torch.FloatTensor(discounted_ep_rs).reshape(-1,1).to(self.device)\n",
        "        # discounted_ep_rs_norm = (discounted_ep_rs-discounted_ep_rs.mean())/(discounted_ep_rs.std()+1e-5)\n",
        " \n",
        "        # 输出网络计算出的每个动作的概率值\n",
        "        act_logit = self.actor(obs)\n",
        "        \n",
        "        # 进行交叉熵的运算\n",
        "        neg_log_prob = F.cross_entropy(input = act_logit,target=torch.LongTensor(self.ep_act).to(self.device),reduction = 'none')\n",
        "        # neg_log_prob = torch.sum(-F.log_softmax(act_logit) * torch.nn.functional.one_hot(torch.LongTensor(self.ep_act).to(self.device), self.act_dim), dim=1)\n",
        "        # 计算loss\n",
        "        \n",
        "\n",
        "        actor_loss = torch.mean(neg_log_prob * (discounted_ep_rs-v))\n",
        "        value_loss = F.mse_loss(self.value(obs), discounted_ep_rs)\n",
        "\n",
        "\n",
        "        # 反向传播优化网络\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        self.value_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        value_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        self.value_optimizer.step()\n",
        "\n",
        "        # 每次学习后清空s,r,a的数组\n",
        "        self.ep_r,self.ep_act,self.ep_obs = [],[],[]\n",
        "\n",
        "\n",
        "EPISODE = 3000\n",
        "RENDER = False\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = ShortCorridor()\n",
        "    # 初始化RF类\n",
        "    agent = ReinforceBaselineAgent()\n",
        "    # 进行训练\n",
        "    reward_total =[]\n",
        "    for episode in range(EPISODE):\n",
        "            env.reset()\n",
        "            obs = env.state\n",
        "            rewards = 0\n",
        "            step_count = 0\n",
        "            while True:\n",
        "                # 与环境的交互\n",
        "                step_count += 1\n",
        "                action = agent.choose_action(obs)\n",
        "                reward,done = env.step(action)\n",
        "                next_obs = env.state\n",
        "                # 存储一个episode中每个step的s,a,r\n",
        "                agent.store_transition(obs,action,reward)\n",
        "                # 进入下一个状态\n",
        "                obs = next_obs\n",
        "                rewards += reward\n",
        "                # 每个episode结束再进行训练(MC)\n",
        "                if RENDER and (episode+1) % 100 == 0 and episode !=0:\n",
        "                    env.render(episode,step_count)\n",
        "                if done:\n",
        "                    agent.update()\n",
        "                    break\n",
        "            reward_total.append(rewards)\n",
        "            # 每100个episode进行测试\n",
        "            if (episode+1) % 100 == 0 and episode !=0:\n",
        "                print('Episode: ',episode+1,'rewards: ',np.mean(reward_total[-100:]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgUeKZGh-OYw"
      },
      "source": [
        "**Question 3.1: Evaluation with different learning rate in REINFORCE**\n",
        "\n",
        "In this part, we will show the performance of REINFORCE on the short-corridor gridworld with different hyperparameter $\\alpha$ settings as {2e-3, 2e-4, 2e-5}. The number of trials is 100, and the number of episode is 1000. Discount rate is  $\\gamma=1$.\n",
        "\n",
        "You need to plot the results which show the total reward on episode with these three different learning rates, which is averaged over 100 runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_return(alpha_list,reward_alpha,num_episodes):\n",
        "    reward_alpha = reward_alpha.reshape(-1,1)\n",
        "    print('length of episodes: ', num_episodes)\n",
        "\n",
        "    fig = plt.figure()\n",
        "    for i in range(len(alpha_list)):\n",
        "        plt.plot(np.arange(1, num_episodes+1), reward_alpha[i], label=f\"alpha={alpha_list[i]}\")\n",
        "    plt.legend() \n",
        "    plt.ylabel('return')\n",
        "    plt.xlabel('Episodes #')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1U_Aqopl8smj"
      },
      "outputs": [],
      "source": [
        "class PolicyNet(torch.nn.Module):\n",
        "    def __init__(self,obs_dim,act_dim,hidden_dim):\n",
        "        super(PolicyNet,self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(obs_dim,hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim,act_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "        y = F.relu(self.fc1(x))\n",
        "        return self.fc2(y)\n",
        "\n",
        "class ReinforceAgent():\n",
        "    \"\"\"\n",
        "    ReinforceAgent that follows algorithm\n",
        "    'REINFORNCE Monte-Carlo Policy-Gradient Control (episodic)'\n",
        "    \"\"\"\n",
        "\n",
        "        ##############################################################################\n",
        "        #                              ENTER YOUR CODE                               #\n",
        "        ##############################################################################\n",
        "\n",
        "    def __init__(self,lr_actor,gamma):\n",
        "        # 环境的状态和动作维度\n",
        "        self.obs_dim = 1\n",
        "        self.act_dim = 2\n",
        "        self.hidden_dim = 16\n",
        "        self.gamma = gamma\n",
        "        self.device =torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
        "    \"cpu\")\n",
        "        # 存放每个episode的s,a,r\n",
        "        self.ep_obs,self.ep_act,self.ep_r = [],[],[]\n",
        "        # 初始化神经网络\n",
        "        self.actor = PolicyNet(obs_dim = self.obs_dim,act_dim = self.act_dim, hidden_dim = self.hidden_dim).to(self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
        "        # self.time_step = 0\n",
        "        \n",
        "    \n",
        "    def choose_action(self,obs):\n",
        "        obs = torch.tensor([obs],dtype=torch.float).to(self.device)   # 转换为torch的格式\n",
        "        prob = F.softmax(self.actor(obs),dim=0)\n",
        "        action = np.random.choice(range(self.act_dim),p=prob.detach().cpu().numpy())   # 根据softmax输出的概率来选择动作\n",
        "        return action\n",
        "    \n",
        "    # 存储一个episode的状态、动作和回报的函数\n",
        "    def store_transition(self,obs,act,r):\n",
        "        self.ep_obs.append(obs)\n",
        "        self.ep_act.append(act)\n",
        "        self.ep_r.append(r)\n",
        "\n",
        "    # 更新策略网络的函数\n",
        "    def update(self):\n",
        "        # self.time_step += 1  # 记录走过的step\n",
        "        # 记录Gt的值\n",
        "        discounted_ep_rs = np.zeros_like(self.ep_r,dtype = float)\n",
        "        running_add = 0\n",
        "        # 计算未来总收益\n",
        "        for t in reversed(range(0,len(self.ep_r))):  # 反向计算\n",
        "            running_add = running_add * self.gamma + self.ep_r[t]\n",
        "            discounted_ep_rs[t] = running_add\n",
        "\n",
        "        discounted_ep_rs -= np.mean(discounted_ep_rs)  # 减均值\n",
        "        discounted_ep_rs /= np.std(discounted_ep_rs)  # 除以标准差\n",
        "        discounted_ep_rs = torch.FloatTensor(discounted_ep_rs).to(self.device)\n",
        "\n",
        "        # 输出网络计算出的每个动作的概率值\n",
        "        act_logit = self.actor(torch.FloatTensor(self.ep_obs).reshape(-1,1).to(self.device))\n",
        "        \n",
        "        # 进行交叉熵的运算\n",
        "        neg_log_prob = F.cross_entropy(input = act_logit,target=torch.LongTensor(self.ep_act).to(self.device),reduction = 'none')\n",
        "        # neg_log_prob = torch.sum(-F.log_softmax(act_logit) * torch.nn.functional.one_hot(torch.LongTensor(self.ep_act).to(self.device), self.act_dim), dim=1)\n",
        "        # 计算loss\n",
        "        loss = torch.mean(neg_log_prob * discounted_ep_rs)\n",
        "\n",
        "        # 反向传播优化网络\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # 每次学习后清空s,r,a的数组\n",
        "        self.ep_r,self.ep_act,self.ep_obs = [],[],[]\n",
        "\n",
        "\n",
        "RENDER = False\n",
        "\n",
        "def Q3_1():\n",
        "    num_trials = 100\n",
        "    num_episodes = 1000\n",
        "    gamma = 1\n",
        "    alpha_list = [2e-3, 2e-4, 2e-5]\n",
        "    ##############################################################################\n",
        "    #                             ENTER YOUR CODE                               #\n",
        "    ##############################################################################\n",
        "    env = ShortCorridor()\n",
        "    # 初始化ReinforceAgent类\n",
        "    \n",
        "    # 进行训练\n",
        "    reward_alpha = np.zeros(len(alpha_list))\n",
        "    reward_total =[]\n",
        "    for alpha in alpha_list:\n",
        "        agent = ReinforceAgent(alpha,gamma)\n",
        "        with tqdm(total=int(num_trials), desc='alpha= %d' % alpha) as pbar:\n",
        "            for trial in range(num_trials):\n",
        "                reward_total_episode = []\n",
        "                for episode in range(num_episodes):\n",
        "                        env.reset()\n",
        "                        obs = env.state\n",
        "                        rewards = 0\n",
        "                        step_count = 0\n",
        "                        while True:\n",
        "                            # 与环境的交互\n",
        "                            step_count += 1\n",
        "                            action = agent.choose_action(obs)\n",
        "                            reward,done = env.step(action)\n",
        "                            next_obs = env.state\n",
        "                            # 存储一个episode中每个step的s,a,r\n",
        "                            agent.store_transition(obs,action,reward)\n",
        "                            # 进入下一个状态\n",
        "                            obs = next_obs\n",
        "                            rewards += reward\n",
        "                            # 每个episode结束再进行训练(MC)\n",
        "                            if RENDER and (episode+1) % 100 == 0 and episode !=0:\n",
        "                                env.render(episode,step_count)\n",
        "                            if done:\n",
        "                                agent.update()\n",
        "                                break     \n",
        "                        reward_total_episode.append(rewards)\n",
        "                reward_total.append(reward_total_episode)\n",
        "                pbar.set_postfix({'trial': '%d' % trial})\n",
        "                pbar.update(1)\n",
        "            reward_total_average = np.array(reward_total).reshape(num_trials,-1).mean(axis=0)\n",
        "            reward_alpha[alpha_list.index(alpha)] = reward_total_average\n",
        "    \n",
        "    plot_return(alpha_list,reward_alpha,num_episodes)    \n",
        "    \n",
        "\n",
        "                        # 每100个episode进行测试\n",
        "                        # if (episode+1) % 100 == 0 and episode !=0:\n",
        "                        #     avg_reward = np.mean(reward_total[-100:])\n",
        "                        #     print('Episode: ',episode+1,'rewards: ',avg_reward)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EEkueO1UGEIx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "alpha= 0:  67%|██████▋   | 67/100 [05:15<02:44,  4.98s/it, trial=66]"
          ]
        }
      ],
      "source": [
        "Q3_1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGV728g4_5sy"
      },
      "source": [
        "**Question 3.2: Evaluation of REINFORCE w/o baseline**\n",
        "\n",
        "In this part, we will show the performance of REINFORCE and REINFORCE-with-baseline on the short-corridor gridworld. The hyperparameter $\\alpha$ for REINFORCE is set as {2e-4}. The hyperparameter $\\alpha, \\alpha^w$ for REINFORCE with baseline is set as {2e-3, 2e-2}. The number of trials is 100, and the number of episode is 1000. Discount rate is  $\\gamma=1$.\n",
        "\n",
        "You need to plot the results which show the total reward on episode with these three different learning rates, which is averaged over 100 runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWroWKif9iqc"
      },
      "outputs": [],
      "source": [
        "class PolicyNet(torch.nn.Module):\n",
        "    def __init__(self,obs_dim,act_dim,hidden_dim):\n",
        "        super(PolicyNet,self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(obs_dim,hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim,act_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "        y = F.relu(self.fc1(x))\n",
        "        return self.fc2(y)\n",
        "\n",
        "class ValueNet(torch.nn.Module):\n",
        "    def __init__(self,obs_dim,hidden_dim):\n",
        "        super(ValueNet,self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(obs_dim,hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        y = F.relu(self.fc1(x))\n",
        "        return self.fc2(y)\n",
        "\n",
        "\n",
        "class ReinforceBaselineAgent():\n",
        "        ##############################################################################\n",
        "        #                             ENTER YOUR CODE                               #\n",
        "        ##############################################################################\n",
        "    def __init__(self,alpha,alpha_actor,gamma):\n",
        "        # 环境的状态和动作维度\n",
        "        self.obs_dim = 1\n",
        "        self.act_dim = 2\n",
        "        self.hidden_dim = 16\n",
        "        self.gamma = gamma\n",
        "        self.device =torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
        "    \"cpu\")\n",
        "        # 存放每个episode的s,a,r\n",
        "        self.ep_obs,self.ep_act,self.ep_r = [],[],[]\n",
        "        # 初始化神经网络\n",
        "        self.actor = PolicyNet(obs_dim = self.obs_dim,act_dim = self.act_dim, hidden_dim = self.hidden_dim).to(self.device)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=alpha_actor)\n",
        "        self.value = ValueNet(obs_dim = self.obs_dim,hidden_dim = self.hidden_dim).to(self.device)\n",
        "        self.value_optimizer = torch.optim.Adam(self.value.parameters(), lr=alpha)\n",
        "\n",
        "        # self.time_step = 0\n",
        "        \n",
        "    \n",
        "    def choose_action(self,obs):\n",
        "        obs = torch.tensor([obs],dtype=torch.float).to(self.device)   # 转换为torch的格式\n",
        "        prob = F.softmax(self.actor(obs),dim=0)\n",
        "        action = np.random.choice(range(self.act_dim),p=prob.detach().cpu().numpy())   # 根据softmax输出的概率来选择动作\n",
        "        return action\n",
        "    \n",
        "    # 存储一个episode的状态、动作和回报的函数\n",
        "    def store_transition(self,obs,act,r):\n",
        "        self.ep_obs.append(obs)\n",
        "        self.ep_act.append(act)\n",
        "        self.ep_r.append(r)\n",
        "\n",
        "    # 更新策略网络的函数\n",
        "    def update(self):\n",
        "        # self.time_step += 1  # 记录走过的step\n",
        "        # 记录Gt的值\n",
        "        obs = torch.FloatTensor(self.ep_obs).reshape(-1,1).to(self.device)\n",
        "        discounted_ep_rs = np.zeros_like(self.ep_r,dtype = float)\n",
        "        running_add = 0\n",
        "        # 计算未来总收益\n",
        "        for t in reversed(range(0,len(self.ep_r))):  # 反向计算\n",
        "            running_add = running_add * self.gamma + self.ep_r[t]\n",
        "            discounted_ep_rs[t] = running_add\n",
        "        v = self.value(obs)\n",
        "        # v = (v-v.mean())/(v.std()+1e-5)\n",
        "        discounted_ep_rs = torch.FloatTensor(discounted_ep_rs).reshape(-1,1).to(self.device)\n",
        "        # discounted_ep_rs_norm = (discounted_ep_rs-discounted_ep_rs.mean())/(discounted_ep_rs.std()+1e-5)\n",
        " \n",
        "        # 输出网络计算出的每个动作的概率值\n",
        "        act_logit = self.actor(obs)\n",
        "        \n",
        "        # 进行交叉熵的运算\n",
        "        neg_log_prob = F.cross_entropy(input = act_logit,target=torch.LongTensor(self.ep_act).to(self.device),reduction = 'none')\n",
        "        # neg_log_prob = torch.sum(-F.log_softmax(act_logit) * torch.nn.functional.one_hot(torch.LongTensor(self.ep_act).to(self.device), self.act_dim), dim=1)\n",
        "        # 计算loss\n",
        "        \n",
        "\n",
        "        actor_loss = torch.mean(neg_log_prob * (discounted_ep_rs-v))\n",
        "        value_loss = F.mse_loss(self.value(obs), discounted_ep_rs)\n",
        "\n",
        "\n",
        "        # 反向传播优化网络\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        self.value_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        value_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        self.value_optimizer.step()\n",
        "\n",
        "        # 每次学习后清空s,r,a的数组\n",
        "        self.ep_r,self.ep_act,self.ep_obs = [],[],[]\n",
        "\n",
        "def Q3_2():\n",
        "    num_trials = 100\n",
        "    num_episodes = 1000\n",
        "    alpha_actor = 2e-4\n",
        "    gamma = 1\n",
        "    alpha_w = [2e-3,2e-2]\n",
        "    RENDER = False\n",
        "\n",
        "    ##############################################################################\n",
        "    #                             ENTER YOUR CODE                               #\n",
        "    ##############################################################################\n",
        "    env = ShortCorridor()\n",
        "    # 初始化ReinforceAgent类\n",
        "    \n",
        "    # 进行训练\n",
        "    reward_alpha = np.zeros(len(alpha_w))\n",
        "    reward_total =[]\n",
        "    for alpha in alpha_w:\n",
        "        agent = ReinforceBaselineAgent(alpha,alpha_actor,gamma)\n",
        "        with tqdm(total=int(num_trials), desc='alpha= %d' % alpha) as pbar:\n",
        "            for trial in range(num_trials):\n",
        "                reward_total_episode = []\n",
        "                for episode in range(num_episodes):\n",
        "                        env.reset()\n",
        "                        obs = env.state\n",
        "                        rewards = 0\n",
        "                        step_count = 0\n",
        "                        while True:\n",
        "                            # 与环境的交互\n",
        "                            step_count += 1\n",
        "                            action = agent.choose_action(obs)\n",
        "                            reward,done = env.step(action)\n",
        "                            next_obs = env.state\n",
        "                            # 存储一个episode中每个step的s,a,r\n",
        "                            agent.store_transition(obs,action,reward)\n",
        "                            # 进入下一个状态\n",
        "                            obs = next_obs\n",
        "                            rewards += reward\n",
        "                            # 每个episode结束再进行训练(MC)\n",
        "                            if RENDER and (episode+1) % 100 == 0 and episode !=0:\n",
        "                                env.render(episode,step_count)\n",
        "                            if done:\n",
        "                                agent.update()\n",
        "                                break     \n",
        "                        reward_total_episode.append(rewards)\n",
        "                reward_total.append(reward_total_episode)\n",
        "                pbar.set_postfix({'trial': '%d' % trial})\n",
        "                pbar.update(1)\n",
        "            reward_total_average = np.array(reward_total).reshape(num_trials,-1).mean(axis=0)\n",
        "            reward_alpha[alpha_w.index(alpha)] = reward_total_average\n",
        "    \n",
        "    plot_return(alpha_w,reward_alpha,num_episodes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKRfuqewF_Je"
      },
      "outputs": [],
      "source": [
        "Q3_2()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
